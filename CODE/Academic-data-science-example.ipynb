{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook:\n",
    "\n",
    "## a realistic dataset: predicting academic performance\n",
    "- we use Pandas dataframes to handle the training set\n",
    "\n",
    "## - multi-class problem\n",
    "\n",
    "## - > 2 features\n",
    "\n",
    "## - Overfitting\n",
    "\n",
    "## - Cross-validation\n",
    "\n",
    "## - Ensemble methods to mitigate overfitting in decision trees: Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import (\n",
    "    LinearSVC,\n",
    "    SVC,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Academic/academic_performance_clean.csv', header=0)\n",
    "pred_feat = 'Class'\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the dataset has the following attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Some attributes are numerical, others are categorical, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['Gender', 'Nationality', 'StageID', 'GradeID', 'SectionID',\n",
    "               'Topic','Semester', 'Relation', 'ParentAnsweringSurvey',\n",
    "               'ParentSchoolSatisfaction', 'StudentAbsenceDays']\n",
    "\n",
    "numerical = ['RaisedHands', 'VisitedResources',\n",
    "             'AnnouncementsView', 'Discussion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the codes for the categorical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in categorical:\n",
    "    print(f\"{v}: {df[v].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we have one distinguished variable, `Class`, which is the category we will want to predict given all other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for categorical variables, we can plot the counts of each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in categorical:\n",
    "    g = sns.FacetGrid(df, col=\"Class\")\n",
    "    g.map(sns.countplot, v, order=sorted(df[v].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some of these deserve better plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.countplot(x=\"Nationality\", data=df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.countplot(x=\"Topic\", data=df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.countplot(x=\"GradeID\", data=df, order=sorted(df['GradeID'].unique()))\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary statistics for numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for numerical variables, we can plot their distribution across the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in numerical:\n",
    "    g = sns.FacetGrid(df)\n",
    "    g.map(sns.histplot, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## it is also interesting to look at the distributions of numerical variables across the two Class outcomes, possibly further aggregated, eg by Gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in numerical:\n",
    "    g = sns.FacetGrid(df, col=\"Class\")\n",
    "    g.map(sns.barplot, \"Gender\", v, order=[\"Male\", \"Female\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can also easily switch the type of plots to achieve different visualisations:\n",
    "\n",
    "hint: try replacing the `kind` with one of `box`, `boxen`, `violin`, `point`, `bar`, `swarm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in numerical:\n",
    "    g = sns.catplot(x=\"Class\", y=v, data=df, kind=\"box\", hue='Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one-hot encoding \n",
    "### classifiers require that categorical variables be encoded in a specific way.\n",
    "\n",
    "we are going to create *dummy variables* for each of these variables. \n",
    "we can either:\n",
    "\n",
    "- assign a numeric value to each categorical value in a set, eg GradeID\n",
    "- generates one new column for each value of a variable, see eg Nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"Nationality\", \"SectionID\", \"Topic\"]\n",
    "\n",
    "for c in col:\n",
    "    dummies = []\n",
    "    dummies.append(pd.get_dummies(df[c]))\n",
    "    df_dummies = pd.concat(dummies, axis=1)\n",
    "    df = pd.concat((df, df_dummies), axis=1)\n",
    "    df = df.drop([c], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Gender: 0 Female, 1 Male\n",
    "- StageID: 0 Lower, 1 Middle, 2 High\n",
    "- GradeID: 0 - 11\n",
    "- Semester: 0 First, 1 Second\n",
    "- Relation: 0 Mother, 1 Father\n",
    "- ParentAnswering: 0 No, 1 Yes\n",
    "- ParentSchool: 0 Bad, 1 Good\n",
    "- Class: 0 L, 1 M, 2 H\n",
    "- Topic: \n",
    "- StudentAbsence: 0 Under-7, 1 Above-7\n",
    "- Nationality:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload the dataset ready for processing\n",
    "df = pd.read_csv('../data/Academic/academic_onehot.csv', header=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### values for each of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCategorical = [col for col in df.columns if col not in numerical]\n",
    "\n",
    "for v in newCategorical:\n",
    "    print(f\"{v}: {df[v].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we now check that the class values are not *unbalanced*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[pred_feat].value_counts()\n",
    "\n",
    "#  0 L, 1 M, 2 H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.countplot(x=\"Class\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class = 1 (Medium) twice the size of each of the other two... let us rebalance.\n",
    "\n",
    "since we have few data points, we amplify the minority classes using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbalanced train set\n",
    "X = df.drop([pred_feat], axis=1).values\n",
    "CL = df[pred_feat].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebalance through upsampling\n",
    "X_reb, CL_reb = SMOTE().fit_resample(X, CL)\n",
    "\n",
    "print(f\"Unbalanced Counts:\\n{Counter(CL)}\\n\")\n",
    "print(f\"Balanced Counts:\\n{Counter(CL_reb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in train / tet\n",
    "XTrain, XTest, CLTrain, CLTest = train_test_split(X_reb, CL_reb, test_size = 0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Set Counts:\\n{Counter(CLTrain)}\\n\")\n",
    "print(f\"Test Set Counts:\\n{Counter(CLTest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaling\n",
    "scaler = StandardScaler()\n",
    "XTrain, XTest  = (scaler.fit_transform(d)\n",
    "                  for d in (XTrain, XTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first attempt at modelling: linear model using _logistic regression_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', C=1, solver='lbfgs',multi_class='auto', max_iter=10000)\n",
    "clf.fit(XTrain, CLTrain)\n",
    "\n",
    "# predictions on training set\n",
    "CL_pred_Train = clf.predict(XTrain)\n",
    "\n",
    "# predictions on test set\n",
    "CL_pred_Test = clf.predict(XTest)\n",
    "\n",
    "print('Train Set Predictions Report:\\n')\n",
    "print(classification_report(CLTrain, CL_pred_Train))\n",
    "print('Test Set Predictions Report:\\n')\n",
    "print(classification_report(CLTest, CL_pred_Test))\n",
    "\n",
    "scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n",
    "print(f\"Cross-validation scores: {scores.mean():.2f} (± {scores.std() * 2:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "note that logit has one hyper-parameter: `C`\n",
    "\n",
    "can we improve on performance by tuning this _hyper-parameter_?\n",
    "\n",
    "how do we select the /optimal/ values for these hyper-parameters?\n",
    "\n",
    "*note*: the code below is from the scikit doc on [Parameter estimation using grid search with cross-validation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'C': [1, 10, 100]}]\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=10000), tuned_parameters, cv=5)\n",
    "clf.fit(XTrain, CLTrain)\n",
    "\n",
    "print(\"Best parameters set found on development set:\\n\")\n",
    "print(clf.best_params_)\n",
    "print(\"\\nGrid scores on development set:\\n\")\n",
    "\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(f\"{mean:.3f} (± {std * 2:.03f}) for {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second attempt: SVM linear  (Support Vector Machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(C=1, max_iter=10000)\n",
    "clf = svm.fit(XTrain, CLTrain)\n",
    "\n",
    "# predictions on training set\n",
    "CL_pred_Train = clf.predict(XTrain)\n",
    "\n",
    "# predictions on test set\n",
    "CL_pred_Test = clf.predict(XTest)\n",
    "    \n",
    "print('Train Set Predictions Report:\\n')\n",
    "print(classification_report(CLTrain, CL_pred_Train))\n",
    "print('Test Set Predictions Report:\\n')\n",
    "print(classification_report(CLTest, CL_pred_Test))\n",
    "\n",
    "scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n",
    "print(f\"Cross-validation scores: {scores.mean():.2f} (± {scores.std() * 2:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a non-linear classifier: SVM with polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='poly')\n",
    "clf.fit(XTrain, CLTrain)\n",
    "\n",
    "CL_pred_Train = clf.predict(XTrain)\n",
    "CL_pred_Test = clf.predict(XTest)\n",
    "\n",
    "print('Train Set Predictions Report:\\n')\n",
    "print(classification_report(CLTrain, CL_pred_Train))\n",
    "print('Test Set Predictions Report:\\n')\n",
    "print(classification_report(CLTest, CL_pred_Test))\n",
    "\n",
    "scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n",
    "print(f\"Cross-validation scores: {scores.mean():.2f} (± {scores.std() * 2:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(XTrain, CLTrain)\n",
    "\n",
    "CL_pred_Train = clf.predict(XTrain)\n",
    "CL_pred_Test = clf.predict(XTest)\n",
    "\n",
    "print('Train Set Predictions Report:\\n')\n",
    "print(classification_report(CLTrain, CL_pred_Train))\n",
    "print('Test Set Predictions Report:\\n')\n",
    "print(classification_report(CLTest, CL_pred_Test))\n",
    "\n",
    "scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n",
    "print(f\"Cross-validation scores: {scores.mean():.2f} (± {scores.std() * 2:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the hyper-parameters by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'kernel': ['rbf'], 'C': [1, 10, 100]},\n",
    "                    {'kernel': ['poly'], 'C': [1, 10, 100]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100]}]\n",
    "\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, cv=5)\n",
    "clf.fit(XTrain, CLTrain)\n",
    "\n",
    "print(\"Best parameters set found on development set:\\n\")\n",
    "print(clf.best_params_)\n",
    "print(\"\\nGrid scores on development set:\\n\")\n",
    "\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(f\"{mean:.3f} (± {std * 2:.03f}) for {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decision trees are attractive but may overfit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=10)\n",
    "clf = clf.fit(XTrain, CLTrain)\n",
    "\n",
    "# predictions\n",
    "CL_pred_Train = clf.predict(XTrain)\n",
    "CL_pred_Test = clf.predict(XTest)\n",
    "\n",
    "print('Train Set Predictions Report\\n')\n",
    "print(classification_report(CLTrain, CL_pred_Train))\n",
    "print('Test Set Predictions Report\\n')\n",
    "print(classification_report(CLTest, CL_pred_Test))\n",
    "\n",
    "scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n",
    "print(f\"Cross-validation scores: {scores.mean():.2f} (± {scores.std() * 2:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to mitigate overfitting, we introduce _ensemble models_: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, max_depth=50, random_state=seed)\n",
    "clf.fit(XTrain, CLTrain)\n",
    "\n",
    "# predictions\n",
    "CL_pred_Train = clf.predict(XTrain)\n",
    "CL_pred_Test = clf.predict(XTest)\n",
    "\n",
    "print('Train Set Predictions Report:\\n')\n",
    "print(classification_report(CLTrain, CL_pred_Train))\n",
    "print('Test Set Predictions Report:\\n')\n",
    "print(classification_report(CLTest, CL_pred_Test))\n",
    "\n",
    "scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n",
    "print(f\"Cross-validation scores: {scores.mean():.2f} (± {scores.std() * 2:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let us tune the hyper-parameters `max_depth` and  `n_estimators`\n",
    "\n",
    "ref: https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "\n",
    "A search consists of:\n",
    "\n",
    "- an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    "- a parameter space;\n",
    "- a method for searching or sampling candidates;\n",
    "- a cross-validation scheme; and\n",
    "- a score function.\n",
    "\n",
    "By default, parameter search uses the score function of the estimator to evaluate a parameter setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = ['precision', 'recall']\n",
    "\n",
    "tuned_parameters = [{'n_estimators': [10,50,100,200],\n",
    "                     'max_depth': [5,10,20,50]}]\n",
    "clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5)\n",
    "clf.fit(XTrain, CLTrain)\n",
    "\n",
    "print(\"Best parameters set found on development set:\\n\")\n",
    "print(clf.best_params_)\n",
    "print(\"\\nGrid scores on development set:\\n\")\n",
    "\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(f\"{mean:.3f} (± {std * 2:.03f}) for {params}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
