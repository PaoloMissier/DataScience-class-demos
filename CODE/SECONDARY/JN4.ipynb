{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"name":"Copy of JN4.ipynb","provenance":[{"file_id":"https://github.com/Trotts/csc2034-ds-demos/blob/master/CODE/JN4.ipynb","timestamp":1618138185026}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Oy7ckdTejSsj"},"source":["# In this notebook:\n","\n","## a realistic dataset: predicting academic performance\n","- we use Pandas dataframes to handle the training set\n","\n","## - multi-class problem\n","\n","## - > 2 features\n","\n","## - Overfitting\n","\n","## - Cross-validation\n","\n","## - Ensemble methods to mitigate overfitting in decision trees: Random Forests"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"S599g5EpjSsm"},"source":["\n","from imblearn.over_sampling import SMOTE\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","\n","import matplotlib.pyplot as plt\n","\n","\n","import urllib.request\n","\n","# Imports commonFunctions from github. \n","## NOTE: THIS IS A SECURITY RISK IF YOU HAD TO DO THIS IN REAL LIFE\n","!pip install httpimport\n","import httpimport\n","\n","path = 'https://raw.githubusercontent.com/Trotts/csc2034-ds-demos/master/CODE/'\n","with httpimport.remote_repo(['commonFunctions'], path):\n","    from commonFunctions import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j3qSqlJJjSsn"},"source":["!pip install imblearn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ataka6khjSsn"},"source":["path = 'https://raw.githubusercontent.com/Trotts/csc2034-ds-demos/master/DATA/'\n","df = pd.read_csv(path + 'Academic/academic_performance_clean.csv', header=0)\n","pred_feat = 'Class'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_WXGnSx_jSso"},"source":["## the dataset has the following attributes:"]},{"cell_type":"code","metadata":{"id":"XALFkWtIjSso"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xoEwkbzdjSso"},"source":[" ## Some attributes are numerical, others are categorical, as follows."]},{"cell_type":"code","metadata":{"id":"yIIDewbmjSsp"},"source":["categorical = ['Gender', 'Nationality', 'StageID', 'GradeID', 'SectionID', 'Topic','Semester', 'Relation', 'ParentAnsweringSurvey', 'ParentSchoolSatisfaction', 'StudentAbsenceDays']\n","\n","numerical = ['RaisedHands', 'VisitedResources', 'AnnouncementsView', 'Discussion']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GmGYoDGbjSsp"},"source":["Here are the codes for the categorical attributes:"]},{"cell_type":"code","metadata":{"id":"XrfN74q-jSsp"},"source":["for v in categorical:\n","    print(\"{v}: {vv}\".format(v=v, vv=df[v].unique()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NpfpMS5xjSsq"},"source":["## we have one distinguished variable, `Class`, which is the category we will want to predict given all other variables"]},{"cell_type":"markdown","metadata":{"id":"O15zz215jSsq"},"source":["## for categorical variables, we can plot the counts of each value:"]},{"cell_type":"code","metadata":{"id":"0vnWOjD2jSsq"},"source":["for v in categorical:\n","    g = sns.FacetGrid(df, col=\"Class\")\n","    g.map(sns.countplot, v, order=sorted(df[v].unique()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69d6e9DXjSsq"},"source":["### Some of these deserve better plots:"]},{"cell_type":"code","metadata":{"id":"2LIIRZvOjSsr"},"source":["g = sns.countplot(x=\"Nationality\", data = df)\n","g.set_xticklabels(g.get_xticklabels(), rotation=90)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0a0wTQJxjSsr"},"source":["g = sns.countplot(x=\"Topic\", data = df)\n","g.set_xticklabels(g.get_xticklabels(), rotation=90)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VivKujsRjSss"},"source":["g = sns.countplot(x=\"GradeID\", data = df, order=sorted(df['GradeID'].unique()))\n","g.set_xticklabels(g.get_xticklabels(), rotation=90)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iEAB8i6zjSss"},"source":["## summary statistics for numerical variables:"]},{"cell_type":"code","metadata":{"id":"EdM03DqVjSss"},"source":["df[numerical].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fMpwKUpsjSst"},"source":["## for numerical variables, we can plot their distribution across the dataset:"]},{"cell_type":"code","metadata":{"id":"VyxjoXcojSst"},"source":["for v in numerical:\n","    g = sns.FacetGrid(df)\n","    g.map(sns.histplot, v)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L2f3anu_jSst"},"source":["## it is also interesting to look at the distributions of numerical variables across the two Class outcomes, possibly further aggregated, eg by Gender:"]},{"cell_type":"code","metadata":{"id":"oJI3Szv9jSst"},"source":["for v in numerical:\n","    g = sns.FacetGrid(df, col=\"Class\")\n","    g.map(sns.barplot, \"Gender\", v, order=[\"Male\", \"Female\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NbP9LTmGjSsu"},"source":["## we can also easily switch the type of plots to achieve different visualisations:\n","\n","hint: try replacing the `kind` with one of `box`, `point`, `bar`, `swarm`"]},{"cell_type":"code","metadata":{"id":"qZaasw62jSsu"},"source":["for v in numerical:\n","    g = sns.catplot(x=\"Class\", y=v, data=df, kind=\"violin\", hue='Gender')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bykOGPwsjSsu"},"source":["# one-hot encoding \n","### classifiers require that categorical variables be encoded in a specific way.\n","\n","we are going to create *dummy variables* for each of these variables. \n","we can either:\n","\n","- assign a numeric value to each categorical value in a set, eg GradeID\n","- generates one new column for each value of a variable, see eg Nationality"]},{"cell_type":"code","metadata":{"id":"C9wrOUZmjSsu"},"source":["col = [\"Nationality\", \"SectionID\", \"Topic\"]\n","\n","for c in col:\n","    dummies = []\n","    dummies.append(pd.get_dummies(df[c]))\n","    df_dummies = pd.concat(dummies, axis = 1)\n","    df = pd.concat((df, df_dummies), axis = 1)\n","    df = df.drop([c], axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BaA3juxSjSsv"},"source":["\n","\n","- Gender: 0 Female, 1 Male\n","- StageID: 0 Lower, 1 Middle, 2 High\n","- GradeID: 0 - 11\n","- Semester: 0 First, 1 Second\n","- Relation: 0 Mother, 1 Father\n","- ParentAnswering: 0 No, 1 Yes\n","- ParentSchool: 0 Bad, 1 Good\n","- Class: 0 L, 1 M, 2 H\n","- Topic: \n","- StudentAbsence: 0 Under-7, 1 Above-7\n","- Nationality:  "]},{"cell_type":"markdown","metadata":{"id":"jq8JM85LjSsv"},"source":["here is the result:"]},{"cell_type":"code","metadata":{"id":"edOsFEf2jSsv"},"source":["## reload the dataset ready for processing\n","df = pd.read_csv('../Academic/academic_onehot.csv', header=0)\n","\n","df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZFrFgx2jSsv"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PeOowPtmjSsv"},"source":["newCategorical = ['Gender', 'StageID', 'GradeID', 'Semester', 'Relation',       'ParentAnsweringSurvey', 'ParentSchoolSatisfaction',\n","       'StudentAbsenceDays', 'Egypt', 'Iran', 'Iraq', 'Jordan',\n","       'Kuwait', 'Lebanon', 'Libya', 'Morocco', 'Palestine', 'SaudiArabia',\n","       'Syria', 'Tunisia', 'USA', 'Venezuela', 'section_A', 'section_B',\n","       'section_C', 'Arabic', 'Biology', 'Chemistry', 'English', 'French',\n","       'Geology', 'History', 'IT', 'Math', 'Quran', 'Science', 'Spanish']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REApPWyRjSsw"},"source":["df.head(20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mbuvvrIbjSsw"},"source":["### values for each of the variables:"]},{"cell_type":"code","metadata":{"id":"V9NDEnMVjSsx"},"source":["for v in newCategorical:\n","    print(\"{v}: {vv}\".format(v=v, vv=df[v].unique()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8Ciy2WkjSsx"},"source":["## we now check that the class values are not *unbalanced*:"]},{"cell_type":"code","metadata":{"id":"gnt-YagNjSsx"},"source":["df[pred_feat].value_counts()\n","\n","#  0 L, 1 M, 2 H"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4HhGlsU1jSsx"},"source":["g = sns.countplot(x=\"Class\", data = df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G-LbpBJgjSsy"},"source":["Class = 1 (Medium) twice the size of each of the other two... let us rebalance.\n","\n","since we have few data points, we amplify the minority classes using SMOTE"]},{"cell_type":"code","metadata":{"id":"jKuDW9f2jSsy"},"source":["# Unbalanced train set\n","X = df.drop([pred_feat], axis=1).values\n","CL = df[pred_feat].values\n","\n","X_reb = X\n","CL_reb = CL"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWQtLPXUjSsy"},"source":["# rebalanced through upsampling\n","X_reb, CL_reb = SMOTE().fit_resample(X, CL)\n","\n","Counter(CL), Counter(CL_reb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kWMsR1CLjSsy"},"source":["# Split data in train / tet\n","XTrain, XTest, CLTrain, CLTest = train_test_split(X_reb, CL_reb, test_size = 0.33, random_state = 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLaMzkBBjSsz"},"source":["Counter(CLTrain), Counter(CLTest)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"axXQB4NBjSsz"},"source":["## scaling\n","XTrain, XTest  = scale(XTrain, XTest)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ikQXN_ijSsz"},"source":["### first attempt at modelling: linear model using _logistic regression_"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"NYFNF-SmjSsz"},"source":["clf = LogisticRegression(penalty='l2', C=1, solver='lbfgs',multi_class='auto')\n","clf.fit(XTrain, CLTrain)\n","\n","# predictions on training set\n","CL_pred_Train = clf.predict(XTrain)\n","\n","# predictions on test set\n","CL_pred_Test = clf.predict(XTest)\n","\n","evaluationReport(CLTrain, CL_pred_Train, CLTest, CL_pred_Test, False)\n","\n","scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n","print(\"Cross-validation scores: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T1vgviXBjSsz"},"source":["## Hyper-parameter tuning"]},{"cell_type":"markdown","metadata":{"id":"1cAlKiMwjSs0"},"source":["\n","note that logit has one hyper-parameter: `C`\n","\n","can we improve on performance by tuning this _hyper-parameter_?\n","\n","how do we select the /optimal/ values for these hyper-parameters?\n","\n","*note*: the code below is from the scikit doc on [Parameter estimation using grid search with cross-validation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html)"]},{"cell_type":"code","metadata":{"id":"1z0IgWDdjSs5"},"source":["# Set the parameters by cross-validation\n","tuned_parameters = [{'C': [1, 10, 100]}]\n","\n","clf = GridSearchCV(LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=500), tuned_parameters, cv=5)\n","clf.fit(XTrain, CLTrain)\n","\n","print(\"Best parameters set found on development set:\")\n","print()\n","print(clf.best_params_)\n","print()\n","print(\"Grid scores on development set:\")\n","print()\n","means = clf.cv_results_['mean_test_score']\n","stds = clf.cv_results_['std_test_score']\n","for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n","    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yrVCthiLjSs6"},"source":["### Second attempt: SVM linear  (Support Vector Machines)"]},{"cell_type":"code","metadata":{"id":"VnyplawSjSs6"},"source":["svm = LinearSVC(C=1, max_iter=5000)\n","clf = svm.fit(XTrain, CLTrain)\n","\n","# predictions on training set\n","CL_pred_Train = clf.predict(XTrain)\n","\n","# predictions on test set\n","CL_pred_Test = clf.predict(XTest)\n","    \n","evaluationReport(CLTrain, CL_pred_Train, CLTest, CL_pred_Test, False)\n","\n","scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n","print(\"Cross-validation scores: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDBY9TqvjSs6"},"source":["### Let's try a non-linear classifier: SVM with polynomial kernel"]},{"cell_type":"code","metadata":{"id":"1fR_TPJ4jSs7"},"source":["clf, CL_pred_Train, CL_pred_Test  = SVM(XTrain, CLTrain, XTest, CLTest, kernel='poly')\n","\n","evaluationReport(CLTrain, CL_pred_Train, CLTest, CL_pred_Test, False)\n","\n","scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n","print(\"Cross-validation scores: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cHiUQ99RjSs7"},"source":["## SVM with RBF kernel"]},{"cell_type":"code","metadata":{"id":"iQ4l4SqKjSs7"},"source":["clf, CL_pred_Train, CL_pred_Test  = SVM(XTrain, CLTrain, XTest, CLTest, kernel='rbf')\n","\n","evaluationReport(CLTrain, CL_pred_Train, CLTest, CL_pred_Test, False)\n","\n","scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n","print(\"Cross-validation scores: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uqMgYaD9jSs8"},"source":["## Tune the hyper-parameters by cross-validation"]},{"cell_type":"code","metadata":{"id":"w9b443xRjSs8"},"source":["tuned_parameters = [{'kernel': ['rbf'], 'C': [1, 10, 100]},\n","                    {'kernel': ['poly'], 'C': [1, 10, 100]},\n","                    {'kernel': ['linear'], 'C': [1, 10, 100]}]\n","\n","clf = GridSearchCV(SVC(), tuned_parameters, cv=5)\n","clf.fit(XTrain, CLTrain)\n","\n","print(\"Best parameters set found on development set:\")\n","print()\n","print(clf.best_params_)\n","print()\n","print(\"Grid scores on development set:\")\n","print()\n","means = clf.cv_results_['mean_test_score']\n","stds = clf.cv_results_['std_test_score']\n","for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n","    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8i8wi5DjSs8"},"source":["## decision trees are attractive but may overfit the data"]},{"cell_type":"code","metadata":{"id":"DUN5iYapjSs9"},"source":["clf = tree.DecisionTreeClassifier(max_depth=10)\n","clf = clf.fit(XTrain, CLTrain)\n","\n","# predictions\n","CL_pred_Train = clf.predict(XTrain)\n","CL_pred_Test = clf.predict(XTest)\n","\n","evaluationReport(CLTrain, CL_pred_Train, CLTest, CL_pred_Test, False)\n","\n","scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n","print(\"Cross-validation scores: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2WlL8hq8jSs9"},"source":["from sklearn.model_selection import cross_val_score\n","    \n","clf, CL_pred_Train, CL_pred_Test  = SVM(XTrain, CLTrain, XTest, CLTest, kernel='rbf')\n","\n","evaluationReport(CLTrain, CL_pred_Train, CLTest, CL_pred_Test, False)\n","\n","scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n","print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rp5HEx06jSs9"},"source":["## to mitigate overfitting, we introduce _ensemble models_: Random Forest"]},{"cell_type":"code","metadata":{"id":"eM5JK002jSs9"},"source":["\n","\n","clf = RandomForestClassifier(n_estimators=10, max_depth=50, random_state=0)\n","clf.fit(XTrain, CLTrain)\n","\n","# predictions\n","CL_pred_Train = clf.predict(XTrain)\n","CL_pred_Test = clf.predict(XTest)\n","\n","evaluationReport(CLTrain, CL_pred_Train, CLTest, CL_pred_Test, False)\n","\n","scores = cross_val_score(clf, XTrain, CLTrain, cv=3)\n","print(\"Cross-validation scores: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aMATkHqDjSs-"},"source":["## let us tune the hyper-parameters `max_depth` and  `n_estimators`\n","\n","ref: https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n","\n","A search consists of:\n","\n","- an estimator (regressor or classifier such as sklearn.svm.SVC());\n","- a parameter space;\n","- a method for searching or sampling candidates;\n","- a cross-validation scheme; and\n","- a score function.\n","\n","By default, parameter search uses the score function of the estimator to evaluate a parameter setting."]},{"cell_type":"code","metadata":{"id":"afkEdEK2jSs-"},"source":["# scores = ['precision', 'recall']\n","\n","tuned_parameters = [{'n_estimators': [10,50,100,200], 'max_depth': [5,10,20,50]}]\n","clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5)\n","clf.fit(XTrain, CLTrain)\n","\n","print(\"Best parameters set found on development set:\")\n","print()\n","print(clf.best_params_)\n","print()\n","print(\"Grid scores on development set:\")\n","print()\n","means = clf.cv_results_['mean_test_score']\n","stds = clf.cv_results_['std_test_score']\n","for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n","    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"feNmpf3ujSs-"},"source":[""],"execution_count":null,"outputs":[]}]}